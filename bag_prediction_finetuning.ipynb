{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"./data/\"\n",
    "resize = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    datapath + \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((resize, resize)),\n",
    "        transforms.RandomResizedCrop(resize, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]))\n",
    "test_dataset = ImageFolder(\n",
    "    datapath + \"test\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((resize, resize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(pretrained=True)\n",
    "\n",
    "for p in net.parameters():\n",
    "    p.requires_grid=False\n",
    "    \n",
    "fc_input_dim = net.fc.in_features\n",
    "net.fc = nn.Linear(fc_input_dim, 2)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, data_loader, device=\"cpu\"):\n",
    "    net.eval()\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = net(x).argmax(1)\n",
    "        ys.append(y)\n",
    "        ypreds.append(y_pred)\n",
    "    ys = torch.cat(ys)\n",
    "    ypreds = torch.cat(ypreds)\n",
    "    acc = (ys == ypreds).float().sum() / len(ys)\n",
    "    return acc.item()\n",
    "\n",
    "train_losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "def train_net(net, train_loader, test_loader, only_fc=True,\n",
    "              optimizer_cls=optim.Adam, loss_fn=nn.CrossEntropyLoss(),\n",
    "              num_epoch=10, device=\"cpu\"):\n",
    "    print(\"使用デバイス:\", device)\n",
    "    if only_fc:\n",
    "        optimizer = optimizer_cls(net.fc.parameters())\n",
    "    else:\n",
    "        optimizer = optimizer_cls(net.parameters())\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        net.train()\n",
    "        n = 0\n",
    "        n_acc = 0\n",
    "        for i, (xx, yy) in tqdm(enumerate(train_loader), total = len(train_loader)):\n",
    "            xx = xx.to(device)\n",
    "            yy = yy.to(device)\n",
    "            h = net(xx)\n",
    "            loss = loss_fn(h, yy)\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item() * xx.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n += len(xx)\n",
    "            y_pred = h.argmax(1)\n",
    "            n_acc += (yy == y_pred).float().sum().item()\n",
    "        train_losses.append(running_loss / len(train_loader.dataset))\n",
    "        train_acc.append(n_acc / n)\n",
    "        val_acc.append(eval_net(net, test_loader, device))\n",
    "        print(\"epoch: {}\\ttrain_loss: {:.3f}\\ttrain_acc: {:.3f}\\tval_acc: {:.3f}\".format(\n",
    "             epoch+1, train_losses[-1], train_acc[-1], val_acc[-1]), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\ttrain_loss: 0.677\ttrain_acc: 0.532\tval_acc: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\ttrain_loss: 0.639\ttrain_acc: 0.660\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:23<00:00, 23.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\ttrain_loss: 0.639\ttrain_acc: 0.702\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\ttrain_loss: 0.605\ttrain_acc: 0.766\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\ttrain_loss: 0.518\ttrain_acc: 0.787\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\ttrain_loss: 0.536\ttrain_acc: 0.702\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\ttrain_loss: 0.503\ttrain_acc: 0.702\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\ttrain_loss: 0.436\ttrain_acc: 0.809\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:23<00:00, 23.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\ttrain_loss: 0.456\ttrain_acc: 0.787\tval_acc: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:27<00:00, 27.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\ttrain_loss: 0.414\ttrain_acc: 0.830\tval_acc: 0.833\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "train_net(net, train_loader, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
